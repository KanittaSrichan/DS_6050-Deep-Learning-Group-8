{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df4657c-538f-4193-a504-024ae0dbfa3b",
   "metadata": {},
   "source": [
    "## Group 4 Deep Learning Project Developing Robust Models for Natural Distribution Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eaebb9-4ffc-4b01-9ae4-f00560f2f400",
   "metadata": {},
   "source": [
    "Please download the dataset from [this link: SDNET2018](https://digitalcommons.usu.edu/all_datasets/48/) before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e8cca34-ab46-4feb-a4ee-4da44a555acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 00:52:29.616597: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 00:52:30.260475: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed  # Import Joblib for parallel processing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9806bc-2b34-42f3-9899-0626b6e7a84d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering images from folder 'D'...\n",
      "Found 13620 images in folder 'D'.\n",
      "\n",
      "Processing images 1 to 5000 of 13620...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images 5001 to 10000 of 13620...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images 10001 to 13620 of 13620...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 13620 images.\n",
      "Filtering images from folder 'P'...\n",
      "Found 24334 images in folder 'P'.\n",
      "\n",
      "Processing images 1 to 5000 of 24334...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images 5001 to 10000 of 24334...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images 10001 to 15000 of 24334...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images 15001 to 20000 of 24334...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing images 20001 to 24334 of 24334...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing 24334 images.\n"
     ]
    }
   ],
   "source": [
    "# Function to assign labels based on filenames\n",
    "def assign_labels(image_paths):\n",
    "    return np.array([1 if 'c' in img.split('/')[1].lower() else 0 for img in image_paths], dtype=np.int32)\n",
    "\n",
    "# Function to load and preprocess a single image\n",
    "def load_image_from_zip(zip_path, img_path, idx, total_images):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "        with zip_file.open(img_path) as file:\n",
    "            img = Image.open(file)\n",
    "            img = img.resize((64, 64))  # Resize to 64x64\n",
    "            img = img.convert('RGB')    # Ensure image has 3 channels\n",
    "            img = np.array(img) / 255.0  # Normalize pixel values\n",
    "            return img\n",
    "\n",
    "# Function to process images in batches and write to memory-mapped arrays\n",
    "def process_images_in_batches(zip_path, image_filenames, images_mmap_path, labels_mmap_path, batch_size=5000, n_jobs=4):\n",
    "    total_images = len(image_filenames)\n",
    "    image_shape = (64, 64, 3)  # Updated image shape\n",
    "    dtype = 'float32'\n",
    "\n",
    "    # Create memory-mapped arrays for images and labels\n",
    "    images_mmap = np.memmap(images_mmap_path, dtype=dtype, mode='w+', shape=(total_images, *image_shape))\n",
    "    labels_mmap = np.memmap(labels_mmap_path, dtype='int32', mode='w+', shape=(total_images,))\n",
    "\n",
    "    for start_idx in range(0, total_images, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_images)\n",
    "        batch_image_paths = image_filenames[start_idx:end_idx]\n",
    "\n",
    "        print(f\"\\nProcessing images {start_idx + 1} to {end_idx} of {total_images}...\")\n",
    "\n",
    "        # Load images in parallel with a progress bar\n",
    "        images_batch = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(load_image_from_zip)(zip_path, img_path, idx + start_idx, total_images) \n",
    "            for idx, img_path in enumerate(tqdm(batch_image_paths, desc=\"Loading Images\", leave=False))\n",
    "        )\n",
    "        images_batch = np.array(images_batch)\n",
    "\n",
    "        # Assign labels for the batch\n",
    "        labels_batch = assign_labels(batch_image_paths)\n",
    "\n",
    "        # Write to memory-mapped arrays\n",
    "        images_mmap[start_idx:end_idx] = images_batch\n",
    "        labels_mmap[start_idx:end_idx] = labels_batch\n",
    "\n",
    "        # Clean up to free memory\n",
    "        del images_batch\n",
    "        del labels_batch\n",
    "\n",
    "        # Flush changes to disk\n",
    "        images_mmap.flush()\n",
    "        labels_mmap.flush()\n",
    "\n",
    "    # Close memory-mapped arrays\n",
    "    del images_mmap\n",
    "    del labels_mmap\n",
    "    print(f\"Finished processing {total_images} images.\")\n",
    "\n",
    "# Main code to process 'D' and 'P' folders\n",
    "def main():\n",
    "    zip_path = 'SDNET2018.zip'\n",
    "    batch_size = 5000  # Adjust based on available memory\n",
    "    n_jobs = 4         # Adjust based on system capabilities\n",
    "\n",
    "    # Extract all filenames from the zip\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        all_filenames = zip_ref.namelist()\n",
    "\n",
    "    # Function to filter image filenames from a folder\n",
    "    def get_image_filenames(folder):\n",
    "        print(f\"Filtering images from folder '{folder}'...\")\n",
    "        return [f for f in all_filenames if f.startswith(folder) and f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # Process 'D' folder\n",
    "    folder_d = 'D'\n",
    "    image_filenames_d = get_image_filenames(folder_d)\n",
    "    print(f\"Found {len(image_filenames_d)} images in folder '{folder_d}'.\")\n",
    "\n",
    "    process_images_in_batches(\n",
    "        zip_path,\n",
    "        image_filenames_d,\n",
    "        images_mmap_path='images_folder_d_mmap.npy',\n",
    "        labels_mmap_path='labels_folder_d_mmap.npy',\n",
    "        batch_size=batch_size,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # Process 'P' folder\n",
    "    folder_p = 'P'\n",
    "    image_filenames_p = get_image_filenames(folder_p)\n",
    "    print(f\"Found {len(image_filenames_p)} images in folder '{folder_p}'.\")\n",
    "\n",
    "    process_images_in_batches(\n",
    "        zip_path,\n",
    "        image_filenames_p,\n",
    "        images_mmap_path='images_folder_p_mmap.npy',\n",
    "        labels_mmap_path='labels_folder_p_mmap.npy',\n",
    "        batch_size=batch_size,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # Return filenames for further use\n",
    "    return image_filenames_d, image_filenames_p\n",
    "\n",
    "# Run the main function and capture the returned filenames\n",
    "if __name__ == \"__main__\":\n",
    "    image_filenames_d, image_filenames_p = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5332e38-fadf-4ea9-9881-bbfe9467de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data using memory-mapped mode\n",
    "total_images_d = 13620\n",
    "images_d = np.memmap('images_folder_d_mmap.npy', dtype='float32', mode='r', shape=(total_images_d, 64, 64, 3))\n",
    "labels_d = np.memmap('labels_folder_d_mmap.npy', dtype='int32', mode='r', shape=(total_images_d,))\n",
    "\n",
    "total_images_p = 24334\n",
    "images_p = np.memmap('images_folder_p_mmap.npy', dtype='float32', mode='r', shape=(total_images_p, 64, 64, 3))\n",
    "labels_p = np.memmap('labels_folder_p_mmap.npy', dtype='int32', mode='r', shape=(total_images_p,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc2bd84-b6ae-4304-99cd-8ada1ddb7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab11aff6-6647-489f-87aa-3a3df4c66c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stratified split for the 'P' dataset...\n",
      "Performing stratified split for the 'D' dataset...\n",
      "Datasets are ready:\n",
      "Training dataset (combined): <_PrefetchDataset element_spec=(TensorSpec(shape=(32, 64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "Test dataset (P only): <_PrefetchDataset element_spec=(TensorSpec(shape=(32, 64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "Test dataset (D only): <_PrefetchDataset element_spec=(TensorSpec(shape=(32, 64, 64, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32,), dtype=tf.int32, name=None))>\n",
      "Number of samples in the combined training dataset: 30362\n",
      "Training labels distribution (P): Counter({0: 17380, 1: 2086})\n",
      "Test labels distribution (P): Counter({0: 4346, 1: 522})\n",
      "Training labels distribution (D): Counter({0: 9276, 1: 1620})\n",
      "Test labels distribution (D): Counter({0: 2319, 1: 405})\n",
      "Set B training dataset size: 2000 samples\n",
      "Set C training dataset size: 10896 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 01:35:49.115093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31013 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# Define the train-test split ratio\n",
    "train_ratio = 0.8\n",
    "batch_size = 32\n",
    "\n",
    "# Function to stratify and split indices for large memory-mapped arrays\n",
    "def stratified_split_indices(labels, train_ratio, random_state=42):\n",
    "    # Get indices for each class\n",
    "    class_0_indices = np.where(labels == 0)[0]\n",
    "    class_1_indices = np.where(labels == 1)[0]\n",
    "\n",
    "    # Shuffle indices for randomness\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(class_0_indices)\n",
    "    np.random.shuffle(class_1_indices)\n",
    "\n",
    "    # Split the indices into training and testing\n",
    "    train_class_0_idx = int(train_ratio * len(class_0_indices))\n",
    "    train_class_1_idx = int(train_ratio * len(class_1_indices))\n",
    "\n",
    "    train_indices = np.concatenate([\n",
    "        class_0_indices[:train_class_0_idx],\n",
    "        class_1_indices[:train_class_1_idx]\n",
    "    ])\n",
    "    test_indices = np.concatenate([\n",
    "        class_0_indices[train_class_0_idx:],\n",
    "        class_1_indices[train_class_1_idx:]\n",
    "    ])\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "# Apply the stratified split function to both datasets\n",
    "print(\"Performing stratified split for the 'P' dataset...\")\n",
    "train_indices_p, test_indices_p = stratified_split_indices(labels_p, train_ratio)\n",
    "\n",
    "print(\"Performing stratified split for the 'D' dataset...\")\n",
    "train_indices_d, test_indices_d = stratified_split_indices(labels_d, train_ratio)\n",
    "\n",
    "# Function to generate batches dynamically from memory-mapped arrays\n",
    "def data_generator(images, labels, indices, batch_size=32):\n",
    "    num_samples = len(indices)\n",
    "    while True:\n",
    "        # Shuffle the indices for each epoch\n",
    "        np.random.shuffle(indices)\n",
    "        # Yield only full batches\n",
    "        for start_idx in range(0, num_samples - batch_size + 1, batch_size):\n",
    "            batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "            # Read images and labels for this batch\n",
    "            batch_images = images[batch_indices]\n",
    "            batch_labels = labels[batch_indices]\n",
    "            yield batch_images, batch_labels\n",
    "\n",
    "# Create training and testing generators\n",
    "# Generators for the 'P' dataset\n",
    "train_gen_p = data_generator(images_p, labels_p, train_indices_p, batch_size=batch_size)\n",
    "test_gen_p = data_generator(images_p, labels_p, test_indices_p, batch_size=batch_size)\n",
    "\n",
    "# Generators for the 'D' dataset\n",
    "train_gen_d = data_generator(images_d, labels_d, train_indices_d, batch_size=batch_size)\n",
    "test_gen_d = data_generator(images_d, labels_d, test_indices_d, batch_size=batch_size)\n",
    "\n",
    "# Create TensorFlow datasets from generators\n",
    "train_dataset_p = tf.data.Dataset.from_generator(\n",
    "    lambda: train_gen_p,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, 64, 64, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "train_dataset_d = tf.data.Dataset.from_generator(\n",
    "    lambda: train_gen_d,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, 64, 64, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Combine training datasets\n",
    "train_dataset = train_dataset_p.concatenate(train_dataset_d).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Testing datasets\n",
    "test_dataset_p = tf.data.Dataset.from_generator(\n",
    "    lambda: test_gen_p,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, 64, 64, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset_d = tf.data.Dataset.from_generator(\n",
    "    lambda: test_gen_d,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, 64, 64, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Datasets are ready:\")\n",
    "print(\"Training dataset (combined):\", train_dataset)\n",
    "print(\"Test dataset (P only):\", test_dataset_p)\n",
    "print(\"Test dataset (D only):\", test_dataset_d)\n",
    "\n",
    "# Check the number of samples\n",
    "num_samples_p = len(train_indices_p)\n",
    "num_samples_d = len(train_indices_d)\n",
    "total_samples = num_samples_p + num_samples_d\n",
    "\n",
    "print(f\"Number of samples in the combined training dataset: {total_samples}\")\n",
    "\n",
    "# Verify the new class distributions\n",
    "train_labels_p = labels_p[train_indices_p]\n",
    "train_labels_d = labels_d[train_indices_d]\n",
    "test_labels_p = labels_p[test_indices_p]\n",
    "test_labels_d = labels_d[test_indices_d]\n",
    "\n",
    "print(\"Training labels distribution (P):\", Counter(train_labels_p))\n",
    "print(\"Test labels distribution (P):\", Counter(test_labels_p))\n",
    "print(\"Training labels distribution (D):\", Counter(train_labels_d))\n",
    "print(\"Test labels distribution (D):\", Counter(test_labels_d))\n",
    "\n",
    "# Define subset size for Set B (adjust as needed)\n",
    "subset_size_per_class = 1000\n",
    "\n",
    "# Extract indices for class 0 and class 1 from both datasets (P and D)\n",
    "class_0_indices_p = train_indices_p[train_labels_p == 0]\n",
    "class_1_indices_p = train_indices_p[train_labels_p == 1]\n",
    "\n",
    "class_0_indices_d = train_indices_d[train_labels_d == 0]\n",
    "class_1_indices_d = train_indices_d[train_labels_d == 1]\n",
    "\n",
    "# Shuffle indices for randomness\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(class_0_indices_p)\n",
    "np.random.shuffle(class_1_indices_p)\n",
    "np.random.shuffle(class_0_indices_d)\n",
    "np.random.shuffle(class_1_indices_d)\n",
    "\n",
    "# Select a subset of each class from P and D\n",
    "subset_class_0_p = class_0_indices_p[:subset_size_per_class // 2]\n",
    "subset_class_1_p = class_1_indices_p[:subset_size_per_class // 2]\n",
    "\n",
    "subset_class_0_d = class_0_indices_d[:subset_size_per_class // 2]\n",
    "subset_class_1_d = class_1_indices_d[:subset_size_per_class // 2]\n",
    "\n",
    "# Combine subsets to create a balanced training set for Set B\n",
    "train_indices_b_p = np.concatenate([subset_class_0_p, subset_class_1_p])\n",
    "train_indices_b_d = np.concatenate([subset_class_0_d, subset_class_1_d])\n",
    "total_samples_b = len(train_indices_b_p) + len(train_indices_b_d)\n",
    "\n",
    "# Now calculate steps_per_epoch_b\n",
    "steps_per_epoch_b = total_samples_b // batch_size\n",
    "\n",
    "# Calculate the steps per epoch for training and validation\n",
    "steps_per_epoch_p = len(train_indices_p) // batch_size\n",
    "steps_per_epoch_d = len(train_indices_d) // batch_size\n",
    "steps_per_epoch = steps_per_epoch_p + steps_per_epoch_d\n",
    "validation_steps_p = len(test_indices_p) // batch_size\n",
    "validation_steps_d = len(test_indices_d) // batch_size\n",
    "\n",
    "# Function to generate batches for Set B\n",
    "def data_generator_set_b(images_p, images_d, labels_p, labels_d, indices_p, indices_d, batch_size=32):\n",
    "    num_samples_p = len(indices_p)\n",
    "    num_samples_d = len(indices_d)\n",
    "    total_samples = num_samples_p + num_samples_d\n",
    "\n",
    "    images_list = [images_p, images_d]\n",
    "    labels_list = [labels_p, labels_d]\n",
    "    indices_list = [indices_p, indices_d]\n",
    "\n",
    "    while True:\n",
    "        # Combine and shuffle indices\n",
    "        combined_indices = np.concatenate([\n",
    "            np.column_stack((np.zeros(len(indices_p), dtype=int), indices_p)),\n",
    "            np.column_stack((np.ones(len(indices_d), dtype=int), indices_d))\n",
    "        ])\n",
    "        np.random.shuffle(combined_indices)\n",
    "\n",
    "        # Yield only full batches\n",
    "        for start_idx in range(0, total_samples - batch_size + 1, batch_size):\n",
    "            batch_info = combined_indices[start_idx:start_idx + batch_size]\n",
    "\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "\n",
    "            for data_source, idx in batch_info:\n",
    "                data_source = int(data_source)\n",
    "                idx = int(idx)\n",
    "                img = images_list[data_source][idx]\n",
    "                label = labels_list[data_source][idx]\n",
    "                batch_images.append(img)\n",
    "                batch_labels.append(label)\n",
    "\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_labels = np.array(batch_labels)\n",
    "            yield batch_images, batch_labels\n",
    "\n",
    "# Create generators for Set B\n",
    "train_gen_b = data_generator_set_b(\n",
    "    images_p, images_d, labels_p, labels_d,\n",
    "    train_indices_b_p, train_indices_b_d,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create training dataset for Set B using generator\n",
    "train_dataset_b = tf.data.Dataset.from_generator(\n",
    "    lambda: train_gen_b,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, 64, 64, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Use the entire training set from the D dataset for Set C\n",
    "train_dataset_c = train_dataset_d.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Set B training dataset size: {total_samples_b} samples\")\n",
    "print(f\"Set C training dataset size: {len(train_indices_d)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9129109-5456-4658-a6c5-5a68d75bec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 01:35:50.455220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-11-10 01:35:51.532738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcdd7662980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-10 01:35:51.532764: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-11-10 01:35:51.652852: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-10 01:35:52.519313: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 7s 3ms/step - loss: 0.3319 - accuracy: 0.8936 - val_loss: 0.3124 - val_accuracy: 0.8935\n",
      "Epoch 2/2\n",
      "948/948 [==============================] - 3s 3ms/step - loss: 0.2961 - accuracy: 0.9008 - val_loss: 0.2812 - val_accuracy: 0.9089\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.8441\n",
      "Test accuracy on D-only dataset: 0.8441176414489746\n",
      "Evaluating on the shifted test set...\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.2796 - accuracy: 0.9104\n",
      "Test accuracy on shifted dataset (P): 0.9103618264198303\n",
      "Set B training dataset size: 2000 samples\n",
      "Set C training dataset size: 10896 samples\n",
      "Training model on Set B (Small/Diverse)...\n",
      "Epoch 1/2\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6223 - accuracy: 0.7021 - val_loss: 0.3973 - val_accuracy: 0.8949\n",
      "Epoch 2/2\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5788 - accuracy: 0.7046 - val_loss: 0.3458 - val_accuracy: 0.9091\n",
      "Training model on Set C (Small/Standard)...\n",
      "Epoch 1/2\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 0.4273 - accuracy: 0.8511 - val_loss: 0.5441 - val_accuracy: 0.8896\n",
      "Epoch 2/2\n",
      "340/340 [==============================] - 1s 3ms/step - loss: 0.4127 - accuracy: 0.8543 - val_loss: 0.3975 - val_accuracy: 0.9009\n",
      "Evaluating Set B model on D-only test set...\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.8540\n",
      "Test accuracy of Set B model on D-only dataset: 0.8540441393852234\n",
      "Evaluating Set B model on shifted test set...\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.8978\n",
      "Test accuracy of Set B model on shifted dataset (P): 0.8978207111358643\n",
      "Evaluating Set C model on D-only test set...\n",
      "85/85 [==============================] - 0s 2ms/step - loss: 0.4428 - accuracy: 0.8511\n",
      "Test accuracy of Set C model on D-only dataset: 0.8511029481887817\n",
      "Evaluating Set C model on shifted test set...\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.9007\n",
      "Test accuracy of Set C model on shifted dataset (P): 0.9006990194320679\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simplified CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the combined training dataset\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=2, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test_dataset_p,\n",
    "    validation_steps=validation_steps_p\n",
    ")\n",
    "\n",
    "# Evaluate the model on the D-only test set\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_dataset_d, \n",
    "    steps=validation_steps_d\n",
    ")\n",
    "print(f\"Test accuracy on D-only dataset: {test_accuracy}\")\n",
    "\n",
    "# Evaluate the model on the shifted test set (P-only)\n",
    "print(\"Evaluating on the shifted test set...\")\n",
    "shifted_test_loss, shifted_test_accuracy = model.evaluate(\n",
    "    test_dataset_p,\n",
    "    steps=validation_steps_p\n",
    ")\n",
    "print(f\"Test accuracy on shifted dataset (P): {shifted_test_accuracy}\")\n",
    "\n",
    "# Define subset size for Set B (adjust as needed)\n",
    "subset_size_per_class = 1000\n",
    "\n",
    "# Extract indices for class 0 and class 1 from both datasets (P and D)\n",
    "class_0_indices_p = train_indices_p[train_labels_p == 0]\n",
    "class_1_indices_p = train_indices_p[train_labels_p == 1]\n",
    "\n",
    "class_0_indices_d = train_indices_d[train_labels_d == 0]\n",
    "class_1_indices_d = train_indices_d[train_labels_d == 1]\n",
    "\n",
    "# Shuffle indices for randomness\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(class_0_indices_p)\n",
    "np.random.shuffle(class_1_indices_p)\n",
    "np.random.shuffle(class_0_indices_d)\n",
    "np.random.shuffle(class_1_indices_d)\n",
    "\n",
    "# Select a subset of each class from P and D\n",
    "subset_class_0_p = class_0_indices_p[:subset_size_per_class // 2]\n",
    "subset_class_1_p = class_1_indices_p[:subset_size_per_class // 2]\n",
    "\n",
    "subset_class_0_d = class_0_indices_d[:subset_size_per_class // 2]\n",
    "subset_class_1_d = class_1_indices_d[:subset_size_per_class // 2]\n",
    "\n",
    "# Combine subsets to create a balanced training set for Set B\n",
    "train_indices_b_p = np.concatenate([subset_class_0_p, subset_class_1_p])\n",
    "train_indices_b_d = np.concatenate([subset_class_0_d, subset_class_1_d])\n",
    "total_samples_b = len(train_indices_b_p) + len(train_indices_b_d)\n",
    "\n",
    "# Function to generate batches for Set B\n",
    "def data_generator_set_b(images_p, images_d, labels_p, labels_d, indices_p, indices_d, batch_size=32):\n",
    "    num_samples_p = len(indices_p)\n",
    "    num_samples_d = len(indices_d)\n",
    "    total_samples = num_samples_p + num_samples_d\n",
    "\n",
    "    images_list = [images_p, images_d]\n",
    "    labels_list = [labels_p, labels_d]\n",
    "    indices_list = [indices_p, indices_d]\n",
    "\n",
    "    while True:\n",
    "        # Combine and shuffle indices\n",
    "        combined_indices = np.concatenate([\n",
    "            np.column_stack((np.zeros(len(indices_p), dtype=int), indices_p)),\n",
    "            np.column_stack((np.ones(len(indices_d), dtype=int), indices_d))\n",
    "        ])\n",
    "        np.random.shuffle(combined_indices)\n",
    "\n",
    "        # Adjusted loop to exclude incomplete batches\n",
    "        for start_idx in range(0, total_samples - batch_size + 1, batch_size):\n",
    "            batch_info = combined_indices[start_idx:start_idx + batch_size]\n",
    "\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "\n",
    "            for data_source, idx in batch_info:\n",
    "                data_source = int(data_source)\n",
    "                idx = int(idx)\n",
    "                img = images_list[data_source][idx]\n",
    "                label = labels_list[data_source][idx]\n",
    "                batch_images.append(img)\n",
    "                batch_labels.append(label)\n",
    "\n",
    "            batch_images = np.array(batch_images)\n",
    "            batch_labels = np.array(batch_labels)\n",
    "            yield batch_images, batch_labels\n",
    "\n",
    "# Create generators for Set B\n",
    "train_gen_b = data_generator_set_b(\n",
    "    images_p, images_d, labels_p, labels_d,\n",
    "    train_indices_b_p, train_indices_b_d,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create training dataset for Set B using generator\n",
    "train_dataset_b = tf.data.Dataset.from_generator(\n",
    "    lambda: train_gen_b,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(batch_size, 64, 64, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Use the entire training set from the D dataset for Set C\n",
    "train_dataset_c = train_dataset_d.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Set B training dataset size: {total_samples_b} samples\")\n",
    "print(f\"Set C training dataset size: {len(train_indices_d)} samples\")\n",
    "\n",
    "# Training with Set B\n",
    "print(\"Training model on Set B (Small/Diverse)...\")\n",
    "steps_per_epoch_b = total_samples_b // batch_size\n",
    "history_b = model.fit(\n",
    "    train_dataset_b,\n",
    "    epochs=2,  # Adjust as needed\n",
    "    steps_per_epoch=steps_per_epoch_b,\n",
    "    validation_data=test_dataset_p,\n",
    "    validation_steps=validation_steps_p\n",
    ")\n",
    "\n",
    "# Training with Set C\n",
    "print(\"Training model on Set C (Small/Standard)...\")\n",
    "history_c = model.fit(\n",
    "    train_dataset_c,\n",
    "    epochs=2,  # Adjust as needed\n",
    "    steps_per_epoch=steps_per_epoch_d,\n",
    "    validation_data=test_dataset_p,\n",
    "    validation_steps=validation_steps_p\n",
    ")\n",
    "\n",
    "# Validate Set B model on the standard test set (D-only)\n",
    "print(\"Evaluating Set B model on D-only test set...\")\n",
    "test_loss_b_d, test_accuracy_b_d = model.evaluate(\n",
    "    test_dataset_d,\n",
    "    steps=validation_steps_d\n",
    ")\n",
    "print(f\"Test accuracy of Set B model on D-only dataset: {test_accuracy_b_d}\")\n",
    "\n",
    "# Validate Set B model on the shifted test set (P-only)\n",
    "print(\"Evaluating Set B model on shifted test set...\")\n",
    "test_loss_b_shifted, test_accuracy_b_shifted = model.evaluate(\n",
    "    test_dataset_p,\n",
    "    steps=validation_steps_p\n",
    ")\n",
    "print(f\"Test accuracy of Set B model on shifted dataset (P): {test_accuracy_b_shifted}\")\n",
    "\n",
    "# Validate Set C model on the standard test set (D-only)\n",
    "print(\"Evaluating Set C model on D-only test set...\")\n",
    "test_loss_c_d, test_accuracy_c_d = model.evaluate(\n",
    "    test_dataset_d,\n",
    "    steps=validation_steps_d\n",
    ")\n",
    "print(f\"Test accuracy of Set C model on D-only dataset: {test_accuracy_c_d}\")\n",
    "\n",
    "# Validate Set C model on the shifted test set (P-only)\n",
    "print(\"Evaluating Set C model on shifted test set...\")\n",
    "test_loss_c_shifted, test_accuracy_c_shifted = model.evaluate(\n",
    "    test_dataset_p,\n",
    "    steps=validation_steps_p\n",
    ")\n",
    "print(f\"Test accuracy of Set C model on shifted dataset (P): {test_accuracy_c_shifted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
